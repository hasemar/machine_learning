{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN classifier\n",
    "\n",
    "This exploration was produced using a combination of the Introduction to Statistical Learning (ISLR) book and scikit-learn documentation.\n",
    "\n",
    "The K Nearest Neighbors algorithm is a simple classifier algorithm that predicts the label of a new data point based on the the number of training samples closest to it.  \n",
    "\n",
    "The 3 main steps to this algorithm are as follows:\n",
    "* Calculate the distance from the new point to all the training samples\n",
    "* Sort the samples by distance from the new point\n",
    "* Predict the new point label based on the closest training samples\n",
    "    * The closest samples are based on a user defined parameter ***K***, wich is the number of points used to determine the new point's label\n",
    "    \n",
    "The prediction is made as follows: <sub>(from ISLR: equation 2.12, pg. 39)</sub>\n",
    "$$Pr(Y = j\\mid X = x_o) = \\frac{1}{K}\\sum_{i\\ \\in N_o}I(y_i = j)$$ \n",
    "\n",
    "where: $I$ is the indicator variable, $x_o$ is the new data point, $N_o$ is the set of points closest to the new data point\n",
    "\n",
    "The parameter ***K*** has a dramatic affect on the predicted value.\n",
    "<img src=\"knn_pic.JPG\" alt=\"mouse drawing skills\" title=\"KNN K value example\" />\n",
    "\n",
    "From my graph showing a data set with a new data point inserted in, the difference between selecting a K value of 3 vs. a K value of 6 is shown.  If we were to choose K to equal 3, then the algorithm would choose the new data point to be in the blue class. If we set K equal to 6, then the new point belongs to the green class.\n",
    "\n",
    "Choosing a good K value is of high importance to acheiving a good performing model. Low values of K can be too flexible and create an *overfitted* model. A K that is too high will have a high rate of misclassification. \n",
    "\n",
    "To check for overfitting, the model can be tested against a test set of data, while iteratively changing K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset   \n",
    "Since the matlab book uses this dataset with a KNN classifier, I will explore a KNN classifier application here in python. \n",
    "\n",
    "[MNIST dataset website](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "The MNIST dataset is a collection of handwritten numbers.  It has 60 thousand training samples and 10 thousand test examples. The samples are 28x28 px black and white images of numbers.\n",
    "\n",
    "The scikit-learn library has the MNIST dataset built in and provides a way to automatically download it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_data = datasets.fetch_mldata('MNIST Original')   # pulls data from http://mldata.org/ instead of MNIST website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (70000, 784)\n",
      "target shape (70000,)\n"
     ]
    }
   ],
   "source": [
    "print('data shape',mnist_data.data.shape)\n",
    "print('target shape',mnist_data.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is made up of 70,000 samples of images. The images are in the form of a 1-D array of 774 greyscale values from 0 to 255. It is the result of a flattened 28 x 28 pixel image.\n",
    "\n",
    "I think we can check an image out by reshaping a sample and plotting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11debf824e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/hWYJ/bH9gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11debb98550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = np.random.randint(0,70000)                       # selecting a random int between 0 and 70k (just for fun)\n",
    "sample_image = mnist_data.data[n].reshape(28,28)     # reshape the first entry as a 28 x 28 matrix\n",
    "plt.imshow(sample_image,cmap='Greys')                # after much trial and tribulation...\n",
    "plt.title('n = ' + str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model\n",
    "I will first try and \"homebrew\" an algorithm to see if I can get any sort of performance from it. My plan is to use the 3 main steps mentioned above, but first I need to split up the data into a test set and a training set.\n",
    "\n",
    "### algorithms within algorithms\n",
    "After reading some the sklearn documentation about KNN, it turns out there are a few different algorithms for determining the distance between observations.  The first and most strait forward is the brute-force method, which literally just takes the difference between each point. This is what I was planning on doing for the homebrew algorithm.  However, the documentation also says that this is only effective for samples of 30 or less.  I have 60,000.\n",
    "\n",
    "I will try to take a set of 30 random samples and implement the homebrew method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[ 1.  5.  3.  1.  1.  4.  8.  1.  9.  1.  2.  5.  6.  4.  2.  7.  8.  6.\n",
      "  9.  8.  0.  6.  8.  5.  5.  4.  2.  0.  6.  3.  0.  5.  5.  7.  3.  5.\n",
      "  1.  4.  9.  9.  4.  0.  1.  0.  7.  3.  4.  0.  8.  3.  1.  2.  7.  5.\n",
      "  3.  1.  1.  2.  3.  6.  3.  2.  7.  0.  0.  8.  9.  8.  3.  6.  9.  1.\n",
      "  6.  3.  7.  0.  3.  9.  1.  8.  1.  3.  2.  0.  8.  4.  6.  7.  0.  3.\n",
      "  2.  1.  0.  5.  9.  8.  2.  2.  9.  7.  5.  1.  6.  9.  2.  3.  7.  7.\n",
      "  4.  7.  5.  8.  2.  6.  3.  4.  8.  1.  6.  8.  6.  1.  4.  6.  2.  2.\n",
      "  9.  3.  5.  3.  5.  4.  1.  4.  1.  8.  5.  5.  7.  8.  1.  7.  7.  4.\n",
      "  6.  1.  8.  1.  0.  9.  3.  5.  8.  7.  2.  9.  3.  7.  6.  9.  3.  2.\n",
      "  0.  9.  1.  2.  6.  9.  4.  2.  8.  8.  8.  1.  1.  1.  8.  3.  1.  8.\n",
      "  4.  6.  5.  7.  3.  8.  1.  6.  7.  4.  5.  3.  6.  7.  8.  2.  8.  7.\n",
      "  1.  5.  9.  7.  3.  3.  0.  5.  3.  7.  8.  7.  5.  4.  8.  3.  4.  4.\n",
      "  7.  2.  0.  0.  1.  3.  5.  0.  3.  5.  2.  3.  2.  6.  1.  4.  5.  9.\n",
      "  5.  4.  5.  3.  3.  9.  5.  1.  2.  3.  1.  6.  1.  5.  7.  4.  6.  3.\n",
      "  5.  8.  4.  4.  7.  7.  6.  3.  1.  2.  9.  1.  6.  0.  0.  7.  8.  6.\n",
      "  0.  1.  6.  6.  3.  4.  1.  4.  1.  1.  2.  6.  8.  4.  1.  6.  1.  3.\n",
      "  8.  1.  8.  5.  3.  4.  2.  0.  9.  1.  7.  3.  3.  1.  1.  3.  4.  0.\n",
      "  4.  7.  9.  3.  8.  3.  1.  1.  6.  7.  7.  3.  2.  5.  5.  6.  4.  9.\n",
      "  9.  9.  6.  6.  4.  1.  1.  8.  5.  2.  7.  7.  8.  2.  0.  0.  4.  1.\n",
      "  2.  4.  3.  0.  3.  3.  7.  6.  5.  5.  1.  8.  1.  5.  7.  5.  8.  0.\n",
      "  0.  8.  3.  7.  5.  2.  4.  9.  5.  2.  3.  4.  0.  9.  5.  2.  8.  3.\n",
      "  6.  7.  3.  2.  1.  0.  2.  9.  3.  7.  4.  2.  9.  1.  5.  3.  8.  4.\n",
      "  9.  0.  7.  9.  1.  6.  7.  9.  5.  6.  3.  0.  6.  1.  2.  4.  9.  5.\n",
      "  7.  6.  7.  9.  5.  4.  7.  8.  9.  7.  1.  0.  2.  1.  3.  7.  3.  3.\n",
      "  3.  9.  3.  8.  5.  3.  5.  5.  0.  1.  6.  6.  1.  5.  2.  0.  4.  2.\n",
      "  4.  7.  6.  0.  2.  3.  3.  3.  2.  7.  7.  4.  6.  7.  2.  6.  1.  5.\n",
      "  1.  8.  0.  7.  2.  4.  6.  2.  0.  7.  1.  0.  8.  8.  9.  6.  0.  1.\n",
      "  3.  0.  9.  4.  6.  3.  0.  5.  8.  0.  3.  7.  4.  1.  3.  7.  3.  3.\n",
      "  4.  4.  9.  8.  3.  7.  9.  8.  9.  6.  2.  7.  5.  8.  5.  5.  6.  2.\n",
      "  1.  2.  7.  1.  8.  4.  1.  1.  2.  9.  9.  7.  6.  6.  0.  7.  2.  5.\n",
      "  7.  0.  2.  3.  5.  1.  3.  9.  9.  8.  5.  6.  8.  6.  6.  6.  9.  1.\n",
      "  0.  5.  5.  6.  8.  8.  7.  1.  3.  7.  8.  3.  7.  6.  7.  4.  2.  2.\n",
      "  0.  8.  9.  4.  5.  2.  3.  5.  5.  4.  2.  0.  5.  0.  2.  3.  6.  2.\n",
      "  2.  9.  1.  8.  2.  6.  6.  3.  9.  0.  5.  4.  8.  7.  1.  7.  9.  3.\n",
      "  6.  4.  5.  8.  2.  5.  1.  1.  1.  5.  9.  4.  9.  3.  1.  0.  2.  8.\n",
      "  9.  0.  2.  1.  1.  7.  4.  7.  0.  0.  1.  4.  2.  8.  0.  0.  7.  9.\n",
      "  0.  2.  0.  3.  3.  1.  9.  2.  2.  1.  1.  0.  0.  5.  9.  8.  0.  2.\n",
      "  8.  7.  7.  9.  2.  6.  8.  3.  7.  5.  5.  8.  2.  4.  1.  5.  1.  2.\n",
      "  5.  0.  6.  8.  2.  4.  0.  9.  1.  6.  8.  5.  8.  7.  6.  3.  3.  9.\n",
      "  8.  7.  1.  4.  6.  0.  4.  7.  0.  4.  3.  1.  5.  9.  8.  9.  9.  1.\n",
      "  4.  4.  4.  5.  7.  7.  4.  4.  8.  1.  3.  7.  1.  2.  9.  9.  3.  1.\n",
      "  3.  0.  9.  0.  5.  3.  0.  9.  4.  5.  9.  2.  8.  1.  2.  7.  3.  6.\n",
      "  8.  4.  7.  4.  9.  0.  4.  2.  0.  9.  4.  9.  9.  7.  6.  6.  0.  8.\n",
      "  4.  5.  0.  6.  0.  5.  0.  3.  9.  4.  0.  9.  0.  0.  9.  5.  9.  1.\n",
      "  5.  7.  7.  9.  7.  8.  7.  7.  1.  7.  3.  3.  6.  7.  9.  1.  0.  4.\n",
      "  2.  4.  7.  9.  9.  4.  4.  0.  0.  8.  6.  6.  8.  4.  2.  9.  9.  4.\n",
      "  1.  1.  8.  3.  6.  5.  6.  8.  5.  2.  5.  1.  9.  6.  8.  3.  1.  4.\n",
      "  6.  2.  6.  3.  6.  1.  4.  3.  8.  3.  7.  0.  7.  1.  7.  1.  9.  9.\n",
      "  4.  1.  2.  5.  6.  8.  0.  1.  0.  2.  9.  1.  3.  1.  8.  1.  9.  5.\n",
      "  1.  9.  6.  0.  2.  4.  1.  4.  5.  8.  4.  9.  1.  2.  4.  7.  8.  2.\n",
      "  0.  5.  6.  3.  1.  9.  8.  8.  5.  2.  8.  3.  6.  0.  0.  7.  6.  8.\n",
      "  9.  5.  8.  4.  4.  0.  6.  4.  4.  5.  7.  8.  2.  1.  9.  7.  1.  9.\n",
      "  8.  5.  9.  4.  2.  6.  5.  8.  0.  6.  9.  0.  2.  7.  3.  9.  7.  5.\n",
      "  7.  5.  6.  6.  1.  9.  5.  6.  5.  2.  5.  0.  0.  6.  1.  5.  2.  2.\n",
      "  9.  5.  5.  8.  3.  3.  3.  9.  7.  0.  9.  0.  3.  0.  2.  4.  8.  5.\n",
      "  0.  7.  9.  8.  1.  4.  6.  6.  2.  1.]\n"
     ]
    }
   ],
   "source": [
    "samples = 1000\n",
    "toy_X = np.zeros((samples,784))\n",
    "toy_y = np.zeros(samples)\n",
    "for rows in range(samples):\n",
    "    i = np.random.randint(0,60000)\n",
    "    toy_X[rows] = mnist_data.data[i]\n",
    "    toy_y[rows] = mnist_data.target[i]\n",
    "print(toy_X)\n",
    "print(toy_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sk-learn has a little function that performs a split of the data in a random manner and is standard practice for building ML models in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting data up into sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(toy_X, toy_y, test_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 1) Distances\n",
    "Calculate the distances between the new point and the training points.\n",
    "\n",
    "Since our observations are not just a scalar value, we cannot simply take the difference between each point. We first have to find the Euclidean distance between each image's \"gray-scale intensity vector\" <--thus deamed\n",
    "\n",
    "The distance is calculated as follows:\n",
    "\n",
    "$d(x,x') = \\sqrt{(x_1-x_1')^2 + (x_2-x_2')^2+...+(x_n-x_n')^2}$\n",
    "\n",
    "where: n = 784 in our case\n",
    "<sub> cite: [How does KNN work?](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/)</sub>\n",
    "\n",
    "Since I am using the most simplistic method (brute force) I need to compare my 10 test samples to my 20 train samples.  This will create a  10x20 matrix of distances, where the rows are test samples, and columns are distance from training sample.\n",
    "\n",
    "*Note:  I tried to use just 30 but their wasn't enough data to even find a mode for the data. I bumped up my training data size to 10000.  So now, I am comparing 10 test samples to 990 training samples* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for euclidean distance\n",
    "def distFun(train,test):\n",
    "    sum = 0\n",
    "    for i in range(len(train)):\n",
    "        sum += (train[i]-test[i])**2\n",
    "    d = np.sqrt(sum)  \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating a list of dictionaries\n",
    "\n",
    "# create main dictionary\n",
    "distances = {}\n",
    "# create sub dictionary\n",
    "dist = {}\n",
    "for row in range(len(X_test)):          # for each test sample \n",
    "    row_dic = {}                       # create a dictionary for the test samples\n",
    "    for col in range(len(X_train)):     # for each training sample\n",
    "        d = distFun(X_test[row],X_train[col])      # compute the distance from the test sample\n",
    "        key = str(row) + str(col)\n",
    "        row_dic[str(col)] = {'dist':d, 'label':y_train[col]}   # put the distance into a dictionary referencing the train label\n",
    "    distances[str(row)] = row_dic                              # add the dictionaries to a dictionary \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary inception..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2851.3537135893889"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances['0']['0']['dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below is just testing indexing and for loop usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test in distances.keys():\n",
    "#     for train in distances[test].keys():\n",
    "#         print(distances[test][train]['dist'], distances[test][train]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) and Step 3)  Sort and predict\n",
    "I can now sort the matrix of distances. I will sort the distances for each row in ascending order.\n",
    "\n",
    "And for efficiency I will calculate the mode of for each test result and make a prediction of the number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuple_list = []\n",
    "test_list = []\n",
    "for test in distances.keys():\n",
    "    for train in distances[test].keys():\n",
    "        tuple_list.append((distances[test][train]['dist'],distances[test][train]['label']))\n",
    "    test_list.append(tuple_list)\n",
    "    tuple_list = []        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2583.4883394356552, 6.0),\n",
       " (2544.6060598843192, 3.0),\n",
       " (2399.6097599401451, 1.0),\n",
       " (2602.0249806640982, 1.0),\n",
       " (2681.959917672149, 7.0)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[9][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can take the ***K*** nearest points from the test values to predict what number it is.\n",
    "\n",
    "Since the test samples are sorted in ascending order, the ***K*** value will corespond to the number of elements we consider, starting from index 0.\n",
    "\n",
    "Then we can check to see what train samples corespond to each element. Our prediction will be based on which train sample is most prevalent in the set of ***K*** values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 1.0, 9.0, 9.0, 6.0, 0.0, 5.0, 7.0, 9.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "k = 40\n",
    "sorted_tpls = []\n",
    "prediction = []\n",
    "tally=[]\n",
    "for tpl_list in test_list:\n",
    "    sorted_tpls = sorted(tpl_list)\n",
    "    for tpls in sorted_tpls:\n",
    "        tally.append(tpls[1])\n",
    "    if len(tally)>0:\n",
    "        #print(tally)\n",
    "        prediction.append(mode(tally[:k]))\n",
    "    sorted_tpls = []\n",
    "    tally=[]\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.  7.  9.  5.  6.  0.  3.  7.  7.  4.]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, actual):\n",
    "    TF_list = []\n",
    "    for elem in range(len(pred)):\n",
    "        if pred[elem] == actual[elem]:\n",
    "            TF_list.append(1)\n",
    "        else:\n",
    "            TF_list.append(0)\n",
    "    perc_correct = sum(TF_list)/len(pred)\n",
    "    return perc_correct*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "score = accuracy(prediction, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whoo hoo!! I made a model that is as good as a coin toss!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
