{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN classifier\n",
    "\n",
    "This exploration was produced using a combination of the Introduction to Statistical Learning (ISLR) book and scikit-learn documentation.\n",
    "\n",
    "The K Nearest Neighbors algorithm is a simple classifier algorithm that predicts the label of a new data point based on the the number of training samples closest to it.  \n",
    "\n",
    "The 3 main steps to this algorithm are as follows:\n",
    "* Calculate the distance from the new point to all the training samples\n",
    "* Sort the samples by distance from the new point\n",
    "* Predict the new point label based on the closest training samples\n",
    "    * The closest samples are based on a user defined parameter ***K***, wich is the number of points used to determine the new point's label\n",
    "    \n",
    "The prediction is made as follows: <sub>(from ISLR: equation 2.12, pg. 39)</sub>\n",
    "$$Pr(Y = j\\mid X = x_o) = \\frac{1}{K}\\sum_{i\\ \\in N_o}I(y_i = j)$$ \n",
    "\n",
    "where: $I$ is the indicator variable, $x_o$ is the new data point, $N_o$ is the set of points closest to the new data point\n",
    "\n",
    "The parameter ***K*** has a dramatic affect on the predicted value.\n",
    "<img src=\"knn_pic.JPG\" alt=\"mouse drawing skills\" title=\"KNN K value example\" />\n",
    "\n",
    "From my graph showing a data set with a new data point inserted in, the difference between selecting a K value of 3 vs. a K value of 6 is shown.  If we were to choose K to equal 3, then the algorithm would choose the new data point to be in the blue class. If we set K equal to 6, then the new point belongs to the green class.\n",
    "\n",
    "Choosing a good K value is of high importance to acheiving a good performing model. Low values of K can be too flexible and create an *overfitted* model. A K that is too high will have a high rate of misclassification. \n",
    "\n",
    "To check for overfitting, the model can be tested against a test set of data, while iteratively changing K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset   \n",
    "Since the matlab book uses this dataset with a KNN classifier, I will explore a KNN classifier application here in python. \n",
    "\n",
    "[MNIST dataset website](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "The MNIST dataset is a collection of handwritten numbers.  It has 60 thousand training samples and 10 thousand test examples. The samples are 28x28 px black and white images of numbers.\n",
    "\n",
    "The scikit-learn library has the MNIST dataset built in and provides a way to automatically download it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.fetch_mldata('MNIST Original')   # pulls data from http://mldata.org/ instead of MNIST website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (70000, 784)\n",
      "target shape (70000,)\n"
     ]
    }
   ],
   "source": [
    "print('data shape',mnist_data.data.shape)\n",
    "print('target shape',mnist_data.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is made up of 70,000 samples of images. The images are in the form of a 1-D array of 774 greyscale values from 0 to 255. It is the result of a flattened 28 x 28 pixel image.\n",
    "\n",
    "I think we can check an image out by reshaping a sample and plotting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2e6deb920b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e6de7b7710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = np.random.randint(0,70000)                       # selecting a random int between 0 and 70k (just for fun)\n",
    "sample_image = mnist_data.data[n].reshape(28,28)     # reshape the first entry as a 28 x 28 matrix\n",
    "plt.imshow(sample_image,cmap='Greys')                # after much trial and tribulation...\n",
    "plt.title('n = ' + str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model\n",
    "I will first try and \"homebrew\" an algorithm to see if I can get any sort of performance from it. My plan is to use the 3 main steps mentioned above, but first I need to split up the data into a test set and a training set.\n",
    "\n",
    "### algorithms within algorithms\n",
    "After reading some the sklearn documentation about KNN, it turns out there are a few different algorithms for determining the distance between observations.  The first and most strait forward is the brute-force method, which literally just takes the difference between each point. This is what I was planning on doing for the homebrew algorithm.  However, the documentation also says that this is only effective for samples of 30 or less.  I have 60,000.\n",
    "\n",
    "I will try to take a set of 30 random samples and implement the homebrew method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[ 3.  6.  3.  2.  5.  1.  6.  8.  9.  3.  5.  6.  2.  5.  1.  3.  7.  0.\n",
      "  5.  7.  2.  1.  6.  2.  2.  1.  2.  7.  2.  0.]\n"
     ]
    }
   ],
   "source": [
    "toy_X = np.zeros((30,784))\n",
    "toy_y = np.zeros(30)\n",
    "for rows in range(30):\n",
    "    i = np.random.randint(0,60000)\n",
    "    toy_X[rows] = mnist_data.data[i]\n",
    "    toy_y[rows] = mnist_data.target[i]\n",
    "print(toy_X)\n",
    "print(toy_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sk-learn has a little function that performs a split of the data in a random manner and is standard practice for building ML models in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting data up into sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(toy_X, toy_y, test_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 1) Distances\n",
    "Calculate the distances between the new point and the training points.\n",
    "\n",
    "Since our observations are not just a scalar value, we cannot simply take the difference between each point. We first have to find the Euclidean distance between each image's \"gray-scale intensity vector\" <--thus deamed\n",
    "\n",
    "The distance is calculated as follows:\n",
    "\n",
    "$d(x,x') = \\sqrt{(x_1-x_1')^2 + (x_2-x_2')^2+...+(x_n-x_n')^2}$\n",
    "\n",
    "where: n = 784 in our case\n",
    "<sub> cite: [How does KNN work?](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/)</sub>\n",
    "\n",
    "Since I am using the most simplistic method (brute force) I need to compare my 10 test samples to my 20 train samples.  This will create a  10x20 matrix of distances, where the rows are test samples, and columns are distance from training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for euclidean distance\n",
    "def distFun(train,test):\n",
    "    sum = 0\n",
    "    for i in range(len(train)):\n",
    "        sum += (train[i]-test[i])**2\n",
    "    d = np.sqrt(sum)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the distance of each test sample to each training sample\n",
    "\n",
    "dist = np.empty((len(X_test),len(X_train)))\n",
    "for row in range(len(X_test)):\n",
    "    for col in range(len(X_train)):\n",
    "        dist[row,col] = distFun(X_test[row],X_train[col])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Sort\n",
    "With my new matrix of distances I can now sort them in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
