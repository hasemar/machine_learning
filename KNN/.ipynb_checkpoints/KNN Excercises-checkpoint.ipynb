{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN classifier\n",
    "\n",
    "This exploration was produced using a combination of the Introduction to Statistical Learning (ISLR) book and scikit-learn documentation.\n",
    "\n",
    "The K Nearest Neighbors algorithm is a simple classifier algorithm that predicts the label of a new data point based on the the number of training samples closest to it.  \n",
    "\n",
    "The 3 main steps to this algorithm are as follows:\n",
    "* Calculate the distance from the new point to all the training samples\n",
    "* Sort the samples by distance from the new point\n",
    "* Predict the new point label based on the closest training samples\n",
    "    * The closest samples are based on a user defined parameter ***K***, wich is the number of points used to determine the new point's label\n",
    "    \n",
    "The prediction is made as follows: <sub>(from ISLR: equation 2.12, pg. 39)</sub>\n",
    "$$Pr(Y = j\\mid X = x_o) = \\frac{1}{K}\\sum_{i\\ \\in N_o}I(y_i = j)$$ \n",
    "\n",
    "where: $I$ is the indicator variable, $x_o$ is the new data point, $N_o$ is the set of points closest to the new data point\n",
    "\n",
    "The parameter ***K*** has a dramatic affect on the predicted value.\n",
    "<img src=\"knn_pic.JPG\" alt=\"mouse drawing skills\" title=\"KNN K value example\" />\n",
    "\n",
    "From my graph showing a data set with a new data point inserted in, the difference between selecting a K value of 3 vs. a K value of 6 is shown.  If we were to choose K to equal 3, then the algorithm would choose the new data point to be in the blue class. If we set K equal to 6, then the new point belongs to the green class.\n",
    "\n",
    "Choosing a good K value is of high importance to acheiving a good performing model. Low values of K can be too flexible and create an *overfitted* model. A K that is too high will have a high rate of misclassification. \n",
    "\n",
    "To check for overfitting, the model can be tested against a test set of data, while iteratively changing K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset   \n",
    "Since the matlab book uses this dataset with a KNN classifier, I will explore a KNN classifier application here in python. \n",
    "\n",
    "[MNIST dataset website](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "The MNIST dataset is a collection of handwritten numbers.  It has 60 thousand training samples and 10 thousand test examples. The samples are 28x28 px black and white images of numbers.\n",
    "\n",
    "The scikit-learn library has the MNIST dataset built in and provides a way to automatically download it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.fetch_mldata('MNIST Original')   # pulls data from http://mldata.org/ instead of MNIST website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (70000, 784)\n",
      "target shape (70000,)\n"
     ]
    }
   ],
   "source": [
    "print('data shape',mnist_data.data.shape)\n",
    "print('target shape',mnist_data.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is made up of 70,000 samples of images. The images are in the form of a 1-D array of 774 greyscale values from 0 to 255. It is the result of a flattened 28 x 28 pixel image.\n",
    "\n",
    "I think we can check an image out by reshaping a sample and plotting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x15c07e7dfd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKYffLFP/D/vOKyDQph/7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15c07d1cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = np.random.randint(0,70000)                       # selecting a random int between 0 and 70k (just for fun)\n",
    "sample_image = mnist_data.data[n].reshape(28,28)     # reshape the first entry as a 28 x 28 matrix\n",
    "plt.imshow(sample_image,cmap='Greys')                # after much trial and tribulation...\n",
    "plt.title('n = ' + str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model\n",
    "I will first try and \"homebrew\" an algorithm to see if I can get any sort of performance from it. My plan is to use the 3 main steps mentioned above, but first I need to split up the data into a test set and a training set.\n",
    "\n",
    "### algorithms within algorithms\n",
    "After reading some the sklearn documentation about KNN, it turns out there are a few different algorithms for determining the distance between observations.  The first and most strait forward is the brute-force method, which literally just takes the difference between each point. This is what I was planning on doing for the homebrew algorithm.  However, the documentation also says that this is only effective for samples of 30 or less.  I have 60,000.\n",
    "\n",
    "I will try to take a set of 30 random samples and implement the homebrew method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[ 4.  6.  7.  9.  6.  2.  3.  6.  2.  3.  2.  5.  8.  2.  8.  8.  6.  5.\n",
      "  7.  7.  7.  0.  9.  0.  4.  9.  5.  8.  2.  1.]\n"
     ]
    }
   ],
   "source": [
    "toy_X = np.zeros((30,784))\n",
    "toy_y = np.zeros(30)\n",
    "for rows in range(30):\n",
    "    i = np.random.randint(0,60000)\n",
    "    toy_X[rows] = mnist_data.data[i]\n",
    "    toy_y[rows] = mnist_data.target[i]\n",
    "print(toy_X)\n",
    "print(toy_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from the MNIST website, it says the data consists of 60,000 training samples and 10,0000 test samples, so I will stick with that.  sk-learn has a little function that performs a split of the data in a random manner and is standard practice for building ML models in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data up into sets\n",
    "testSize = 10000\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist_data.data, mnist_data.target, test_size=testSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 784)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  64,\n",
       "       253, 255,  63,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        96, 205, 251, 253, 205, 111,   4,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  96, 189, 251, 251, 253, 251, 251,  31,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        16,  64, 223, 244, 251, 251, 211, 213, 251, 251,  31,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  80, 181, 251, 253, 251, 251, 251,  94,  96, 251, 251,  31,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  92, 253, 253, 253, 255, 253, 253, 253,  95,  96, 253,\n",
       "       253,  31,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  92, 236, 251, 243, 220, 233, 251, 251, 243,  82,\n",
       "        96, 251, 251,  31,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  80, 253, 251, 251, 188,   0,  96, 251, 251,\n",
       "       109,   0,  96, 251, 251,  31,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  96, 240, 253, 243, 188,  42,   0,  96,\n",
       "       204, 109,   4,   0,  12, 197, 251,  31,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 221, 251, 253, 121,   0,   0,\n",
       "         0,  36,  23,   0,   0,   0,   0, 190, 251,  31,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  48, 234, 253,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0, 191, 253,  31,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  44, 221, 251, 251,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  12, 197, 251,\n",
       "        31,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 190, 251,\n",
       "       251, 251,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  96,\n",
       "       251, 251,  31,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       190, 251, 251, 113,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        40, 234, 251, 219,  23,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0, 190, 251, 251,  94,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  40, 217, 253, 231,  47,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0, 191, 253, 253, 253,   0,   0,   0,   0,   0,\n",
       "         0,  12, 174, 253, 253, 219,  39,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  67, 236, 251, 251, 191, 190, 111,\n",
       "        72, 190, 191, 197, 251, 243, 121,  39,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  63, 236, 251, 253,\n",
       "       251, 251, 251, 251, 253, 251, 188,  94,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  27,\n",
       "       129, 253, 251, 251, 251, 251, 229, 168,  15,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  95, 212, 251, 211,  94,  59,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
